
========================================================

### Multivariate Data
Notes:

***

### Moira Perceived Audience Size Colored by Age
Notes: Okay, so people aren't very good at guessing their audience sizes. But maybe, maybe people who are older, maybe they have a better sense, than teenagers, for example. So the next plot that we did was another scatter plot, but this time we added a third level, where we added color to represent the age of the survey respondent. And you can see again we have this horizontal stripes for people who are guessing that there are 50 or 100 people in their audience. But I don't see any pattern in the color. I think this is actually kind of a dead end.  we do reach dead ends. This is one example of kind of a failure. I can tell if younger people were more accurate than older people, there is too much over plotting. In this plot there are too many dots on top of each other..

***

### Third Qualitative Variable
Notes: Previously we noted that female users have more friends on average than male users. And, we might wonder, .... is this just because female users have a different "age" distribution? Or, maybe c.o.n.d.i.t.i.o.n.a.l. on age, the differences are actually larger. (gender vs friend_count)----- age?

```{r Adding the third Qualitative Variable}
library(ggplot2)
getwd()
list.files()
pf <- read.csv('pseudo_facebook.tsv', sep = '\t') #or USE: read.delim('pseudo_facebook.tsv')
head(pf, 4)


ggplot(aes(x = gender, y = age), data = subset(pf, !is.na(gender))) + geom_boxplot()
#Here's a box plot of ages by gender. Now, I'm going to add the mean for each gender to the box plots, using stat summary. Here we can see the averages (marked by an x since I used shape=4).
ggplot(aes(x=gender, y=age), data = subset(pf, !is.na(gender))) +geom_boxplot() + stat_summary(fun.y=mean, geom='point', shape=4)
#Since male users are a bit younger, we might actually think a simple male to female comparison doesn't capture their substantial differences in friend count. Let's look at median friend count by age and gender instead. 
ggplot(aes(x = age, y = friend_count), data = subset(pf, !is.na(gender))) + 
  geom_line(aes(color=gender), stat = 'summary', fun.y=median)
#we can see that nearly everywhere the median friend count is larger for women than it is for men. Now there are some exceptions, and this includes "these noisy estimates" for our very old users. Now, I'm using old with quotation marks here, since we're not really confident about these reported ages. 
#And notice that users reporting to be of reported gender. You're going to reproduce the same plot, but first let's see if you can create the summary data to create it. Recall that we can produce the same summary data underlying this plot by using the dplyr package. We can divide the data by age and gender and then compute the median and mean friend count for each sub-group. In this next program assignment you're going to do just that by using the group by, summarize, and arrange functions from the dplyr package. 





# Write code to create a new data frame, called 'pf.fc_by_age_gender', that contains information on each age AND gender group.
# The data frame should contain the following variables:
#    mean_friend_count,
#    median_friend_count,
#    n (the number of users in each age and gender grouping)

# Here is an example of the structure of your data frame. Your data values will be different.
#   age gender mean_friend_count median_friend_count    n
# 1  13 female          247.2953                 150  207
# 2  13   male          184.2342                  61  265
# 3  14 female          329.1938                 245  834
# 4  14   male          157.1204                  88 1201

#You can include multiple variables to split the data frame when using group_by() function in the dplyr package.
#new_groupings <- group_by(data, variable1, variable2)
library(dplyr)

age_gender <- group_by(subset(pf, !is.na(gender)), age, gender) 
# Note that if you are grouping by more than one variable, you will probably need to call the ungroup() function. ??????????
pf.fc_by_age_gender <- summarise(age_gender,mean_friend_count = mean(friend_count), median_friend_count = median(friend_count), n=n())
ungroup() #?????????????didnt work?

head(pf.fc_by_age_gender, 5) #so...it creates a...pivot table...

#or
#using chained commands...
#new_data_frame <- data_frame %>%
# group_by(variable1, variable2) %>%
#Repeated use of summarise() and group_by(): The summarize function will automatically remove one level of grouping (the last group it collapsed).
pf.fc_by_age_gender <- pf %>%
  filter(!is.na(gender)) %>%
  group_by(age, gender) %>%
  summarise(mean_friend_count = mean(friend_count), median_friend_count = median(friend_count), n=n()) %>%
  ungroup() %>%
  arrange(age) #Now, summarize will remove one layer of grouping when it runs, so we'll remove the gender layer. So, we need to run ungroup one more time to remove the age layer and finally I'll arrange my data frame by age. 

head(pf.fc_by_age_gender, 5) #so...it creates a...pivot table...

```

***

### Plotting Conditional Summaries
Notes: Now that you've got this data, let's construct this plot that shows the median friend count for each "gender" as "age" increases. 

```{r Plotting Conditional Summaries}
ggplot(aes(x = age, y = median_friend_count), data = pf.fc_by_age_gender) + 
  geom_line(aes(color=gender), stat = 'summary', fun.y=median)
#here, the aes wrapper deals with age variable, and geom_line layer deals with the categorical variable. 
#it helps us understand how the difference between male and female users varies with age.

```

***

### Thinking in Ratios
Notes: it seems like the gender difference is largest for our young users. It would be to put this in relative terms though. So, let's answer a different question. "How many times more friends"" does the average female user have than the male user? Maybe, females have twice as many friends as male users, or maybe it's ten times as many friends.....

***

### Wide and Long Format
Notes: we need to rearrange our data a little bit. Right now, our data is in 'long format'. We have many rows. And, notice how that the variables that we grouped over, male and female, have been repeated. What we need is 'wide format' with values of the median_friend_count..conditional on 'gender' 
#example#
#install.packages('tidyr')
#library(tidyr)
#Wide <- spread(subset(pf.fc_by_age_gender, select=c('gender', 'age', 'median_friend_count')), gender, median_friend_count)

#or....
#library(dplyr)
#Wide <- subset(pf.fc_by_age_gender[c('age', 'gender', 'median_friend_count')], !is.na(gender)) %>%
#  spread(gender, median_friend_count) %>%
#  mutate(ratio = male / female)
***
#or....
### Reshaping Data
Notes:

```{r}
install.packages('reshape2')
library(reshape2)

#coz dataframe..so "d-cast"..
#We specify the data set we are going to change and modify and then we put in a formula.
#Now, the first part of the formula (the left of the tilde sign), will list the variables I want to keep (age). On the right side of the tilde, we use the gender variable since we want male and female users to have their own columns for median friend count in the data frame. And finally, we set "value.var" because it holds the k.e.y m.e.a.s.u.r.e.m.e.n.t.s. or their values in our new dataframe.
pf.fc_by_age_gender2 <- dcast(pf.fc_by_age_gender, age ~ gender, value.var = 'median_friend_count')
head(pf.fc_by_age_gender2, 5)

#later...See if you can recreate these steps on your own and try playing around with the dcast and melt functions, and the reshape to package. The melt function will allow you to convert the wide data back to the original long format.

```

***

### Ratio Plot
Notes:  I want you to plot the ratio of females to males to determine how many times more friends the average female user has, compared to the number of friends the average male user has...Plot the ratio of the female to male median friend counts using the dataframe 'pf.fc_by_age_gender2'.

# Add a horizontal line to the plot with a y intercept of 1, which will be the base line. 
# Use the parameter linetype in geom_hline to make the line dashed. The linetype parameter can take the values 0-6:

# 0 = blank, 1 = solid, 2 = dashed
# 3 = dotted, 4 = dotdash, 5 = longdash
# 6 = twodash

```{r Ratio Plot}
?geom_hline
ggplot(aes(x=age, y=female/male), data = pf.fc_by_age_gender2) + geom_line() + geom_hline(yintercept=1, linetype=2, alpha=0.3)

#We can easily see that for very young users, the median female user has over two and a half times as many friends as the median male user. Clearly, it was helpful to condition on age in understanding the relationship of gender with friend count. This helped assure us this pattern is robust for users of many different ages. And it also highlighted where this difference is most striking. Now, there are many processes that can produce this difference, including the biased distribution from which this pseudo Facebook data was generated. One idea which shows the complexity of interpretation here, is that people from particular countries who more recently joined Facebook are more likely to be male with lower friend counts. 

```

***

### Third Quantitative Variable
Notes: what if we looked at Age and Friend_Count over, say,(not gender? not categorical?) another numerical variable? For example we might notice that since users are likely to accumulate friends over time using Facebook that Facebook'tenure' is important for predicting friend_count. Tenure or how many days since registering with Facebook is associated with age.  ex, The first people to start using Facebook were college students as of 2004 and 2005. He was lucky enough to be part of that group and joined the site on February and have over 1,200 of them. On the other hand, 14 year-old users, have had less time to accumulate the same number of friends. 

One way to explore all four variables friend_count, age, gender and tenure is using a two-dimensional display like a scatter plot. And we can bend one of the 'quantitative variables' and compare those bends. 

In this case, we can group users by the year that they joined. So let's create a new variable called year_joined in our data frame. This variable is going to hold the year that our users first joined Facebook. In our next program and exercise, you're going to create this variable, year_joined, and put it inside the data frame. You need to make use of the variable tenure and use 2014 as the reference year. 

```{r Third Quantitative Variable}
# Create a variable called year_joined in the pf data frame using the variable tenure and 2014 as the reference year.
# The variable year_joined should contain the year that a user joined facebook. Tenure is measured in days, but we want to convert it to years.. Use the floor() function to round down to the nearest integer. Use the ceiling() function to round up to the nearest integer.
# round(2014 + pf$tenure/365) ??
pf$year_joined <-  floor(2014 - pf$tenure/365)

```

***

### Cut a Variable
Notes: converting to the categorical variable!!

```{r Cut a Variable}
summary(pf$year_joined) 
# Most of our users joined in 2012 or 2013 and since the values for this variable are discreet and the range is pretty narrow.. 
table(pf$year_joined)
# Here we can see the distributions of users and each year joined. Notice that there isn't much data here about early joiners. To increase the data we have in each tenure category, we can group some of these years together. 

#The cut() is often quite useful for making "discrete" variables from "continuous or numerical" ones, sometimes in combination with the function quantile. 
?cut #Convert Numeric to Factor..Breaking up a continuous variable such as age into a categorical variable. Or, you might want to classify a categorical variable like year into a larger bin, such as 1990-2000. There are many reasons not to do this when performing regression analysis, but for simple displays of demographic data in tables, it could make sense.

# Create a new variable in the data frame called 'year_joined.bucket' by using the cut function on the variable 'year_joined'.
# You need to create the following buckets for the new variable, 'year_joined.bucket'
#        (2004, 2009]
#        (2009, 2011]
#        (2011, 2012]
#        (2012, 2014]
# Note that a parenthesis means exclude the year and a bracket means include the year.
# Luckily, we can specify the exact intervals we want for age.
# cut(data, breaks = seq(?, ?, by = ?)) but There is no reason that the breaks argument has to be equally spaced
pf$year_joined.bucket <- cut(pf$year_joined, breaks = c(2004,2009,2011,2012,2014)) 

```

***

### Plotting it All Together
Notes:

```{r Plotting it All Together}
# Let's table this new variable to see the distribution in each group.
table(pf$year_joined.bucket, useNA = 'ifany') ########

#Here we can see that we have our four bins of users, depending on when they joined Facebook, and it looks like two people have a value of NA. Let's use this new year_joined.bucket variable to create a line graph Like we did for gender at the start of the lesson. 

# Create a line graph of friend_count vs. age so that each 'year_joined.bucket' is a line tracking the median user friend_count across age. This means you should have four different lines on your plot. 
# You should subset the data to exclude the users whose year_joined.bucket is NA.
ggplot(aes(x=age, y=friend_count), data = subset(pf, !is.na(year_joined.bucket))) + 
  geom_line(aes(color=year_joined.bucket), stat = 'summary', fun.y=median)


```

***

### Plot the Grand Mean
Notes: # Write code to do the following:
# (1) Add another geom_line to code below to plot the grand mean of the friend_count vs age.

# (2) Exclude any users whose year_joined.bucket is NA.

# (3) Use a different line type for the grand mean.

# As a reminder, the parameter linetype can take the values 0-6:

# 0 = blank, 1 = solid, 2 = dashed
# 3 = dotted, 4 = dotdash, 5 = longdash
# 6 = twodash

```{r Plot the Grand Mean}
#Looking at this plot, we can see that our suspicion is confirmed. Users with a longer tenure tend to have higher friend counts, with the exception of our older users, say, about 80 and up. 
#To put these cohort specific medians in perspective, we can change them to cohort specific means. 
#And then plot the grand mean down here as well. 
ggplot(aes(x=age, y=friend_count), data = subset(pf, !is.na(year_joined.bucket))) + 
  geom_line(aes(color=year_joined.bucket), stat = 'summary', fun.y=mean) + 
  geom_line(stat = 'summary', fun.y=mean, linetype=2, alpha=0.8)

####Plotting the grand mean is a good reminder that much of the data in the sample is about members of recent cohorts.#### 
#This way is the type of more high level observation that you want to make as you explore data. 

```

***

### Friending Rate
Notes: Since the general pattern continues to hold after conditioning on each of the 'buckets of year joined', we might increase our confidence that this observation isn't just an artifact of the time that users have had to accumulate friends. 

```{r Friending Rate}
#Let's look at this relationship in another way. We could also look at 'tenure' and 'friend_count' as a rate instead. For example, we could see how many friends does a user have for each day since they've started using the service. 
#Subset the data so you only consider users with at least one day of tenure. Once you have that summary, answer these two questions. What's the median rate? And, what's the maximum rate?

#summary(subset(pf, !is.na(tenure)), friend_count/tenure) #it doesn't work...
#Here we want a summary of the friend rate. So, we can use the 'with()' 
with(subset(pf, tenure>=1), summary(friend_count/tenure))
# the maximum rate is 417. Now, this is definitely an outlier, considering our data, since the third quartile is only about 0.5!

```

***

### Friendships Initiated
Notes: We've seen that users who have been on the site longer, typically have higher friend counts across ages. Now, this leaves me wondering if friend requests are the same or different across groups. Do new users go on friending sprees? Or do users with more tenure initiate more friendships? Let's explore this with a plot. 

# Create a line graph of mean of 'friendships_initiated' per day (of tenure) vs. tenure colored by year_joined.bucket.
# You need to make use of the variables 'tenure', 'friendships_initiated', and 'year_joined.bucket'.
# You also need to subset the data to only consider user with at least one day of tenure.

What is the median friend rate?

What is the maximum friend rate?

```{r Friendships Initiated}
ggplot(aes(x=tenure, y=friendships_initiated/tenure), data = subset(pf, tenure>=1)) +
  geom_line(aes(color=year_joined.bucket), stat = 'summary', fun.y=mean)
# We'll color our line by year_joined.bucket and then we'll plot the mean of the y variable across tenure. 
# Taking a closer look, it appears that users with more tenure typically initiate less friendships. 
# There's a lot of noise in our graph since we are plotting the mean of y for every possible tenure x value. 

```

***

### Bias-Variance Tradeoff Revisited
Notes: we can adjust this noise by bending our x-axis differently. 

```{r Bias-Variance Tradeoff Revisited}
## As the bin size increases we see less noise on the plot!!!
#instead of using tenure here, I'm going to replace this with a different version or formula so that way I can bend some of the tenures together. #original is...
ggplot(aes(x = tenure, y = friendships_initiated / tenure), data = subset(pf, tenure >= 1)) +
  geom_line(aes(color = year_joined.bucket), 
            stat = 'summary', 
            fun.y = mean)
# Binning values by the denominator in the round function and then transforming back to the natural scale with the constant in front. #Now let's see the difference. Notice how I have slightly less noise in my plot.--
ggplot(aes(x = 7 * round(tenure / 7), y = friendships_initiated / tenure), data = subset(pf, tenure > 0)) +
  geom_line(aes(color = year_joined.bucket), 
            stat = "summary", 
            fun.y = mean) 

ggplot(aes(x = 30 * round(tenure / 30), y = friendships_initiated / tenure), data = subset(pf, tenure > 0)) +
  geom_line(aes(color = year_joined.bucket),
            stat = "summary",
            fun.y = mean)

ggplot(aes(x = 90 * round(tenure / 90), y = friendships_initiated / tenure), data = subset(pf, tenure > 0)) +
  geom_line(aes(color = year_joined.bucket),
            stat = "summary",
            fun.y = mean) # very high bias, but much less variance using the number 90. 



# Instead of geom_line(), use geom_smooth() to add a smoother to the plot.
# You can use the defaults for geom_smooth() but do color the line by 'year_joined.bucket.'
# need to get rid of this fun.y parameter and the stat parameter....really?  but what about mean function ?????
ggplot(aes(x = tenure, y = friendships_initiated / tenure), data = subset(pf, tenure >= 1)) +
  geom_smooth(aes(color = year_joined.bucket))

```

***

### Sean's NFL Fan Sentiment Study
Notes: About measuring fan sentiment for NFL teams over the course of the season.. 

I go through all of the emotions that a fan go through over the course of the season. You know, the highs when your team wins, the lows after a couple losses in a row.. feeling kind of hopeless.... I got the idea of .. we could measure this and kind of tell a story. Not just for my team but for all the other teams in the NFL..so I wanted to visualize the experience of being a fan....so I counted ratios of positive to negative words at 'five-minute increments' over the course of the whole 4 months(NFL season). Because we are taking a ratio, we end up with some measurements.  

This was kind of like a first cut at the data, and we can see that there's some signal here. But it's definitely going to need some modeling or some statistics in order to kind of tease out what's happening! 
we're pooling over 'more measurements'(five-minute increment?), so the measurements themselves are more reliable. 

But these measurements are still too frequent and too noisy to tell a story.  When we aggregate it up to one day moving averages, we start to see some patterns emerge. I guess one of the key features of this dataset was that I knew what I was looking for ahead of time. The plot like this immediately tell you this is not telling the story that I want... We wanted a model that predicts sentiment as a function of time. 

#smoothing out#
And one of the things that comes to mind right away is a natural spline ..this actually tells a nice story. These color lines are the dates of wins and loses. It kind of gives you an idea of why the lines are upward sloping or downward sloping. So you can see here, kind of like the exuberance at the beginning of the season as people are really optimistic. And then, you know, three losses in a row and see how the sentiment dips. 
This tells a nice story, but it doesn't have the feature of, that we'd expect, which is that on game day, things change really abruptly. At the very end of the game, when you know if you've won or you've lost, you're much happier or much sadder than you were at the beginning of the game. So we expect to see really discrete jumps in sentiment that we don't see from a model like this. And this is because this is a bad model of the underlying data generating process. 

#discreteness on game days#
So we need something more flexible. One way to do that is just to use a 'seven day moving average' which is going to allow us to include only kind of the last game sentiment in the moving average. So we're going to pick a moving average, like I showed you before. Let's smooth it out into over a whole seven day period. And when we do that, we get something that actually tells a really nice story about the season. And has all the kind of characteristics that I would expect as a fan in having gone through this. Which is, the kind of, the big bumps up on game days where you win, the big bumps down on game days where you lose. And then, kind of, these plateaus in between, which are these periods of stability when you don't have any information about how your team is doing. We see that a week off around Thanksgiving but then there's a big spike in happiness because people are just happy around Thanksgiving. This big low point right after a loss that could have knocked them out of the playoffs. And then a big kind of ascension to their playoff game which they ended up losing and the subsequent dip. This I think is a really nice depiction of what happened and it took a little bit of averaging to get the story to come out.

When you're looking at all this data, what sort of things come up for you in terms of bias and variance tradeoffs? 

When you're computing just a simple moving average like this, you're dealing with one of the most, it's just a really flexible statistic. And so you're, not imposing any structure on the data. You're letting the data kind of speak. When I use a moving average here and I plotted standard errors that were kind of rolling along with the data. They were gigantic. The mean sentiment for the season is somewhere in the three range. And the standard errors were over two or three. We can say very precisely what's happening at any given point. But our variance on that estimate is huge. So as we started to add more lags, higher number of lags to the, moving average. We end up with kind of smoother looking plots, that have lower variance but but then are getting progressively more bias. So as we go back further we are including more data and were getting more bias. Because were including data from parts that actually aren't applicable to that exact point. But in exchange for that we get a lower variance plot, one that doesn't move as wildly. When we combine that with splines, we can end up fitting a model, that has kind of the best of both worlds. Which has the smoothing, aspects of the splines, with the discrete jumps, of what happens on game day. And so this is a spline where we add it dummy variables for post game periods. In this model, we end up with kind of all the same thing. We get the big jumps that we expect, so it jumps down on loses, jumps up on game days where they win. And then also kind of the smooth transitions in between. So it's kind of a nice story of taking one style of model, which is a spine, which is just too specific for the data generating process, and maybe not a good fit. And in doing some exploratory data analysis where we see that averaging over seven days tells a really nice story and gives us the discreteness that we want. And then combining those two together into kind of an aggregate of the two types of models. Where We're able to better account for the fact that game days, are, are an important thing. 

***

### Introducing the Yogurt Data Set
Notes:  Throughout all of our analyses of the pseudo-Facebook user data set, we've come to learn a lot about our users. From their birthdays, to their friend counts, to their friendships initiated, we've really come to understand their behaviors and how they use the Facebook platform. But, now I think it's time for something completely different. In the next couple of segments, we'll look at another data set, and then we'll return to this Facebook data set to draw some comparisons. 

We are going to work with a data set describing household purchases, of five flavors of Dannon yogurt in the eight-ounce size. Their price is recorded with each purchase occasion. 
This yogurt data set has a quite different structure than our pseudo-Facebook data set. The synthetic Facebook data has one row per individual with that row giving their characteristics and counts of behaviors over a single period of time. On the other hand, the yogurt data has many rows per household, one for each purchase occasion. This kind of microdata is often useful for answering different types of questions than we've looked at so far. 

***

### Histograms Revisited
Notes:

```{r Histograms Revisited}
getwd()
list.files()
yo <- read.csv('yogurt.csv')
head(yo, 5)
str(yo)
#Now one thing that you might notice is that most of the variables in here are integers. But I want to convert one of the variables to a factor, and that's the ID variable. 
yo$id <- factor(yo$id)
str(yo)

ggplot(aes(x=price), data=yo) + geom_histogram(color='red')
#We can immediately notice some important discreetness to this distribution. There appear to be prices at which there are many observations, but then no observations in adjacent prices. This makes sense if prices are set in a way that applies to many of the consumers. There are some purchases that involve much lower prices, and if we are interested in price sensitivity, we definitely want to consider what sort of variations is in these prices. Now, I also want you to note that if we chose a different bin width we might obscure this discreetness.

ggplot(aes(x=price), data=yo) + geom_histogram(binwidth = 10, fill='red')
#Say if I chose a bandwidth equal to ten. In this histogram, we would miss the observation for some of the empty spaces for the adjacent prices. So, it's no surprise that for this very discreet data this histogram is a very biased model. 

```

***

### Number of Purchases
Notes: How to detect the discreteness? 
>if we just look at a five number summary of the data, we might not notice this so easily. One clue to the discreteness is that the 75th percentile is the same as the maximum. ==> summary()
>We could also see this discreteness by looking at how many distinct prices there are in the data set. ==> unique()
>Tabling the variable we get an idea of the distribution like we saw in the histogram. ==>table()


```{r Number of Purchases}
summary(yo$price)
unique(yo$price)
length(unique(yo$price))
table(yo$price)

#So now that we know something about the price, let's figure out on a given purchase occasion how many ? yogurts' does a household purchase. To answer this we need to combine the 'counts of the different yogurt flavors' into one variable. For example, for the one particular household, on one purchase occasion, they bought three different types of yogurt. To figure this out for all the households, we need to make use of a new function. 

# Create a new variable called all.purchases,which gives the total counts of yogurt for each observation or household.
# One way to do this is using the 'transform()' 
# The transform function produces a data frame so if you use it then save the result to 'yo'!
# OR you can figure out another way to create the variable.
# yo$all.purchase <- sum(yo[, 4:8]) #....doesn't work..
# transform(data, defining new_variable)

yo <- transform(yo, all.purchase = strawberry+blueberry+pina.colada+plain+mixed.berry)
summary(yo$all.purchase)

ggplot(aes(x=all.purchase), data=yo) + geom_histogram(color='red', binwidth = 1, fill='yellow')

```

***

### Prices over Time
Notes: This histogram reveals that most households buy one or two yogurts at a time. To dive deeper into our yogurt prices and household behavior, let's investigate the price over time in more detail.  In this data set, we can examine changes in prices because we have data on the same households over time. Your visualization should be a scatter plot of price versus time. 

```{r Prices over Time}
## Create a scatterplot of price vs time. This will be an example of a time series plot.
# Resolve overplotting issues...
ggplot(aes(x=time, y=price), data = yo) + geom_point(alpha=0.3, color='red')

ggplot(aes(x=time, y=price), data = yo) + geom_jitter(alpha=0.2, shape=21, fill='yellow')

#Looking at the plot, we can see that the mode or the most common prices, seem to be increasing over time. We also see some lower price points scattered about the graph. These may be due to sales or, perhaps, buyers using coupons that bring down the price of yogurt..

```

***

### Sampling Observations
Notes:  Can you see the scattered time series data? how to proceed differently with this type of a data set?

When familiarizing yourself with a new data set that contains multiple observations of the same units (over time?), it's often useful to work with a sample of those units so that it's easy to display the raw data for that sample. In the case of the yogurt data set, we might want to look at a small sample of households in more detail so that we know what kind of within and between household variation we are working with. This analysis of a sub-sample might come before trying to use within household variation as part of a model. For example, this data set was originally used to model consumer preferences for variety. But, before doing that, we'd want to look at how often we observe households buying yogurt, how often they buy multiple items, and what prices they're buying yogurt at. One way to do this is to look at some sub-sample in more detail. Let's pick 16 households at random and take a closer look...

***

### Looking at Samples of Households

```{r Looking at Sample of Households}
set.seed(123)
sample.ids <- sample(levels(yo$id), 16); sample.ids #randomly picking 16 samples..without replace..
?levels #'id' is factors... otherwise..level() doesnt work.. level..specify..each category in the variable announced as a factor. 

# "x %in% y"" returns a logical (boolean) vector the same length as x that says whether each entry in x appears in y. That is, for each entry in x, it checks to see whether it is in y. 
# This allows us to subset the data so we get all the purchases occasions for the households in the sample. Then, we create scatterplots of price vs. time and facet by the sample id. 

ggplot(aes(x=time, y=price), data = subset(yo, id %in% sample.ids)) + #data is..with 'id'..that only present in 'sample.ids'
         facet_wrap(~id) + #~ variable name' that we want to split the data over..so categorical variable? 
         geom_line() +
         geom_point(aes(size= all.purchase), pch=1) #Use the 'pch' or 'shape' parameter to specify the symbol point...

# From these plots, we can see the variation and how often each household buys yogurt. 
# it seems that some household purchases more quantities than others with these larger circles indicating. For most of the households, the price of yogurt holds steady, or tends to increase over time. Now, there are, of course, some exceptions.  
#we might think that the household is using coupons to drive the price down. Now, we don't have the coupon data to associate with this buying data, but you could see how that information could be paired to this data to better understand the consumer behavior. 

```

***

### The Limits of Cross Sectional Data: the pseudo fascebook data..
Notes: The general idea is that if we have observations over time, we can facet by the primary unit, case, or individual in the dataset. For our yogurt data it was the households we were faceting over. This faceted time series plot is something we can't generate with our pseudo Facebook data set since we don't have data on our sample of users over time. 

The Facebook data isn't great for examining the process of friending over time. The data set is just a cross section, it's just one snapshot at a fixed point that tells us the characteristics of individuals. Not the individuals over, say, a year. But if we had a dataset like the yogurt one, we would be able to track friendships initiated over time and compare that with tenure. This would give us better evidence to explain the difference or the drop in friendships initiated over time as tenure increases. 

***

### Many Variables
Notes: when analyzing the relationship between two variables we look to incorporate more variables in the analysis to improve it. For example, by seeing whether a particular relationship is consistent across values of those other variables. In choosing a third or fourth variable to plot we relied on our domain knowledge. But often, we might want visualizations or summaries to help us identify such auxiliary variables. 
In some analyses, we may plan to make use of a large number of variables. Perhaps, we are planning on predicting one variable with ten, 20, or hundreds of others. Or maybe we want to summarize a large set of variables into a smaller set of dimensions. Or perhaps, we're looking for interesting relationships among a large set of variables. In such cases, we can help speed up our exploratory data analysis by producing many plots or comparisons at once. This could be one way to let the data set as a whole speak in part by drawing our attention to variables we didn't have a preexisting interest in. 

***

### Scatterplot Matrix
Notes: we should let the data speak to determine variables of interest. There's a tool that we can use to create a number of scatter plots automatically. It's called a scatter plot matrix. In a scatter plot matrix. There's a grid of scatter plots between every pair of variables. 

As we've seen, scatter plots are great, but not necessarily suited for all types of variables. For example, categorical ones. So there are other types of visualizations that can be created instead of scatter plots. Like box plots or histograms when the variables are categorical. 

>We need to run the code install.packages('GGally') to install the package for creating this particular scatterplot matrix.
If the plot takes a long time to render or if you want to see some of the scatterplot matrix, then only examine a smaller number of variables. You can use the following code or select fewer variables. We recommend including gender (the 6th variable)!

pf_subset <- pf[, c('age', 'dob_year', 'dob_month', 'gender', 'tenure')]

>You can also select a subset using the subset() function and the "select" argument:

pf_subset <- subset(pf, select = -c(userid, year_joined, year_joined_bucket))

The '-' sign in the "select" value indicates all but the listed columns. You may find in your matrix that variable labels are on the outer edges of the scatterplot matrix, rather than on the diagonal. If you want labels in the diagonal, you can set the 
axisLabels = 'internal'  argument in your ggpairs command.

```{r scatter matrix - ggpairs()}
install.packages("GGally")
library(GGally)
theme_set(theme_minimal(12)) #font size?

#set the seed for reproducible results..
set.seed(123)
str(pf)
pf_sub <- pf[ , c(2:15)] #we dont need userID..
names(pf_sub)
ggpairs(pf_sub[sample.int(nrow(pf_sub), 1000), ]) #only sampling 1000 rows..coz toooo big!

#The GG pairs function uses a different plot type for different types of combinations of variables. Hence, we have histograms here and we have scatter plots here. Many of these plots aren't quite as nice as they would be if we fine-tuned them for the particular variables. For example, for all the counts of likes, we might want to work on a logarithmic scale. But, ggpairs doesn't do this for us. 

```


### Even More Variables
Notes:  This is a genomic data. In these data sets, they're often thousands of genetic measurements for each of a small number of samples. In some cases, some of these samples have a disease, and so we'd like to identify genes that are associated with the disease. 

```{r}
nci <- read.table("nci.tsv")
head(nci, 5)
colnames(nci) <- c(1:64)

```

***

### Heat Maps
Notes: For our dataset, we want to display each combination of gene and sample case, the difference in gene expression and the sample from the base line. We want to display combinations where a gene is overexpressed in red in combinations where a gene is under expressed in blue. 

>First, we'll run all of this in order to melt our data to a long format. And then we just run our ggplot code using the geom, geom tile. Now, this last line is going to give us a scale gradient. And we're going to use the colors from blue to red.

#melt()
:it takes data in wide format and stacks a set of columns into a single column of data. To make use of the function we need to specify a data frame, the id variables (which will be left at their settings) and the measured variables (columns of data) to be stacked. The default assumption on measured variables is that it is all columns that are not specified as id variables.

melt(data, id.vars = 'Factor_name', measure.vars = c("variable(previous col) ", "variable(..)")) # wide format ==> long format

```{r}
library(reshape2)
nci.long.samp <- melt(as.matrix(nci[1:200,]))
names(nci.long.samp) <- c("gene", "case", "value")
head(nci.long.samp)

ggplot(aes(y = gene, x = case, fill = value), data = nci.long.samp) +
  geom_tile() +
  scale_fill_gradientn(colours = colorRampPalette(c("blue", "red"))(100)) #this is a heatmap!

#

```

>Even with such a dense display, we aren't looking at all the data...
 we're just showing the first 200 genes. That's 200 genes of over Genomic data sets of these kind, sometimes called micro data are only getting larger, and more complex. What's most interesting, is that other data sets also look like this. For example, internet companies run lots of randomized experiments. Where in the simplest versions, users are randomly assigned to a treatment like a new version of a website or some sort of new feature or product or a control condition. Then the difference in outcome between the treatment and control can be computed for a number of metrics of interest. In many situations, there might have been hundreds or thousands of experiments and hundreds of metrics. This data looks very  similar to the genomic data in some ways. And this is why the useful maxim plot all the data might not always apply to a data set as it did to most of this course. 

***

### Analyzing Three of More Variables
Reflection:

***

Click **KnitHTML** to see all of your hard work and to have an html
page of this lesson, your answers, and your notes!

